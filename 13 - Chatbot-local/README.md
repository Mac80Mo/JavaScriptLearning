# ğŸ¤– Lokaler ChatBot

Ein eleganter, lokaler Chatbot, der mit Ollama und dem Llama 3.2 Modell arbeitet. Diese Anwendung ermÃ¶glicht es, vollstÃ¤ndig lokal und offline mit einem KI-Modell zu chatten, ohne dass Daten an externe Server gesendet werden.

## ğŸ“¸ Projektvorschau

![Chatbot Screenshot](screenshot.png)

## ğŸ“‹ ProjektÃ¼bersicht

Dieses Projekt implementiert eine benutzerfreundliche Web-OberflÃ¤che fÃ¼r die Kommunikation mit lokalen Large Language Models Ã¼ber die Ollama API. Der Chatbot bietet eine moderne, dunkle BenutzeroberflÃ¤che mit responsivem Design und verschiedenen praktischen Funktionen.

## âœ¨ Features

- **Lokale KI**: VollstÃ¤ndig lokale AusfÃ¼hrung mit Ollama und Llama 3.2
- **Moderne UI**: Elegante, dunkle BenutzeroberflÃ¤che
- **Responsive Design**: Funktioniert auf Desktop und mobilen GerÃ¤ten
- **Intelligentes Scrollen**: Automatisches Scrollen zu neuen Nachrichten
- **Kopierfunktion**: Bot-Antworten kÃ¶nnen per Klick kopiert werden
- **Chat lÃ¶schen**: Conversation kann jederzeit zurÃ¼ckgesetzt werden
- **Scroll-Indikator**: Button zum schnellen Sprung ans Ende des Chats
- **Benutzerfreundlich**: Deutsche OberflÃ¤che mit klaren Fehlermeldungen

## ğŸš€ Verwendete Technologien

- **Frontend**: Vanilla JavaScript, HTML5, CSS3
- **Backend**: Ollama API (localhost:11434)
- **KI-Modell**: Llama 3.2 (Ã¼ber Ollama)
- **Styling**: Custom CSS mit modernem Design

## ğŸ“¦ Voraussetzungen

1. **Ollama installiert**: [Ollama Download](https://ollama.ai/)
2. **Llama 3.2 Modell**:
   ```bash
   ollama pull llama3.2
   ```
3. **Ollama Server gestartet**:
   ```bash
   ollama serve
   ```

## ğŸ› ï¸ Installation & Setup

1. **Repository klonen oder Dateien herunterladen**
2. **Ollama installieren und starten**:

   ```bash
   # Ollama Server starten
   ollama serve

   # In einem neuen Terminal: Llama 3.2 Model herunterladen
   ollama pull llama3.2
   ```

3. **Projekt Ã¶ffnen**: `index.html` im Browser Ã¶ffnen

## ğŸ“ Projektstruktur

```
13 - Chatbot-local/
â”œâ”€â”€ index.html          # Haupt-HTML-Datei
â”œâ”€â”€ script.js           # JavaScript-Logik
â”œâ”€â”€ style.css           # Styling und Layout
â”œâ”€â”€ README.md           # Diese Dokumentation
â””â”€â”€ screenshot.png      # Screenshot der Anwendung
```

## ğŸ’» Verwendung

1. **Ollama starten**: Stellen Sie sicher, dass Ollama lÃ¤uft (`ollama serve`)
2. **Browser Ã¶ffnen**: Ã–ffnen Sie `index.html` in Ihrem bevorzugten Browser
3. **Chatten**: Geben Sie Ihre Frage in das Eingabefeld ein und drÃ¼cken Sie "Fragen"
4. **Antworten kopieren**: Klicken Sie auf das Kopier-Symbol (â§‰) neben Bot-Antworten
5. **Chat leeren**: Verwenden Sie den "LÃ¶schen" Button, um die Conversation zurÃ¼ckzusetzen

## âš™ï¸ Funktionsweise

### Frontend (JavaScript)

- **Nachrichtenverwaltung**: Dynamisches HinzufÃ¼gen von Benutzer- und Bot-Nachrichten
- **API-Kommunikation**: Asynchrone Anfragen an die Ollama API
- **UI-Interaktionen**: Scroll-Management, Kopieren, Chat-Reset
- **Fehlerbehandlung**: Benutzerfreundliche Fehlermeldungen

### Backend (Ollama API)

- **Lokaler Server**: LÃ¤uft auf `localhost:11434`
- **Modell**: Llama 3.2 fÃ¼r natÃ¼rliche Sprachverarbeitung
- **Stream-Modus**: Deaktiviert fÃ¼r vollstÃ¤ndige Antworten
- **Fehlerbehandlung**: Status-Code-basierte Fehlerbehandlung

## ğŸ¨ Design-Features

- **Dunkles Theme**: Moderne, augenfreundliche Farbgebung
- **Responsive Layout**: Anpassung an verschiedene BildschirmgrÃ¶ÃŸen
- **Smooth Scrolling**: FlÃ¼ssige Animationen
- **Benutzer vs. Bot**: Unterschiedliche Styling fÃ¼r verschiedene Nachrichtentypen
- **Hover-Effekte**: Interaktive Elemente mit visueller RÃ¼ckmeldung

## ğŸ”§ Anpassungen

### Modell Ã¤ndern

In `script.js` kÃ¶nnen Sie das verwendete Modell Ã¤ndern:

```javascript
body: JSON.stringify({
  model: "llama3.2:latest", // Hier andere Modelle verwenden
  prompt: question,
  stream: false,
}),
```

### Styling anpassen

Das Design kann Ã¼ber `style.css` vollstÃ¤ndig angepasst werden. Die CSS-Variablen am Anfang der Datei erleichtern FarbÃ¤nderungen.

## ğŸ› Fehlerbehebung

### HÃ¤ufige Probleme:

1. **"Ollama-Server nicht erreichbar"**

   - ÃœberprÃ¼fen Sie, ob Ollama lÃ¤uft: `ollama serve`
   - Port 11434 verfÃ¼gbar?

2. **"Modell konnte nicht geladen werden"**

   - Modell installieren: `ollama pull llama3.2`
   - Modellname Ã¼berprÃ¼fen

3. **Keine Antwort**
   - Internetverbindung fÃ¼r erste Installation
   - GenÃ¼gend RAM verfÃ¼gbar (min. 8GB empfohlen)

## ğŸ“ Entwickelt von

Dieses Projekt ist Teil des JavaScript Campus 10 Kurses und demonstriert:

- Moderne Web-Entwicklung mit Vanilla JavaScript
- API-Integration und asynchrone Programmierung
- Benutzerfreundliches UI/UX Design
- Lokale KI-Integration

## ğŸ“„ Lizenz

Dieses Projekt dient Bildungszwecken und ist frei verwendbar.
